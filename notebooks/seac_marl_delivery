{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!git clone https://github.com/cuongtv312/marl-delivery.git\n%cd marl-delivery\n!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:48:13.598826Z","iopub.execute_input":"2025-05-13T01:48:13.599153Z","iopub.status.idle":"2025-05-13T01:48:20.472527Z","shell.execute_reply.started":"2025-05-13T01:48:13.599124Z","shell.execute_reply":"2025-05-13T01:48:20.470190Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from env import Environment\nimport gymnasium as gym\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:48:47.791162Z","iopub.execute_input":"2025-05-13T01:48:47.791729Z","iopub.status.idle":"2025-05-13T01:48:47.797228Z","shell.execute_reply.started":"2025-05-13T01:48:47.791698Z","shell.execute_reply":"2025-05-13T01:48:47.796038Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=wandb_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T01:48:21.728052Z","iopub.execute_input":"2025-05-13T01:48:21.728363Z","iopub.status.idle":"2025-05-13T01:48:31.536193Z","shell.execute_reply.started":"2025-05-13T01:48:21.728339Z","shell.execute_reply":"2025-05-13T01:48:31.535331Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return LooseVersion(v) >= LooseVersion(check)\n/usr/local/lib/python3.11/dist-packages/google/colab/_import_hooks/_altair.py:16: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n  import imp  # pylint: disable=deprecated-module\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpage0526\u001b[0m (\u001b[33miai-uet-vnu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def reward_shaping(reward, env, next_state, state, actions):\n    shaped_reward = reward\n    robots_before = state['robots']\n    robots_after = next_state['robots']\n    \n    # Track newly picked up packages\n    packages_carried_before = {i: r[2] for i, r in enumerate(robots_before)}\n    packages_carried_after = {i: r[2] for i, r in enumerate(robots_after)}\n    \n    # Convert robot positions to 0-indexed (environment uses 0-indexed internally)\n    robot_positions_before = [(r[0]-1, r[1]-1, r[2]) for r in robots_before]\n    robot_positions_after = [(r[0]-1, r[1]-1, r[2]) for r in robots_after]\n\n    # encourage robot to carry package\n    newly_picked_up = 0\n    for robot_id in packages_carried_before:\n        if packages_carried_before[robot_id] == 0 and packages_carried_after[robot_id] > 0:\n            newly_picked_up += 1\n\n    shaped_reward += 1.0 * newly_picked_up\n    # encourage robot to deliver before deadline\n    delivered_this_step = 0\n    for pkg in env.packages:\n        if pkg.status == 'delivered' and pkg.deadline >= env.t:\n            delivered_this_step += 1\n\n    shaped_reward += 2.0 * delivered_this_step\n\n    # Progress rewards for robots carrying packages\n    for i, (pos_before, pos_after) in enumerate(zip(robot_positions_before, robot_positions_after)):\n        pkg_id = pos_after[2]\n\n        # Skip if not carrying anything\n        if pkg_id == 0:\n            continue\n\n        # Find package target\n        package = next((p for p in env.packages if p.package_id == pkg_id), None)\n        if package:\n            # If robot moved while carrying package\n            if pos_before[:2] != pos_after[:2]:\n                # Calculate distances to target\n                old_dist = manhattan_distance(pos_before[:2], package.target)\n                new_dist = manhattan_distance(pos_after[:2], package.target)\n\n                # Reward progress toward target\n                if new_dist < old_dist:\n                    progress = (old_dist - new_dist) / (old_dist + 0.001)  # Normalized progress\n                    shaped_reward += 0.3 * progress\n                elif new_dist > old_dist:\n                    shaped_reward -= 0.1  # Small penalty for moving away\n\n    for i, pos in enumerate(robot_positions_after):\n        if pos[2] == 0:\n            closest_pkg = None\n            closest_dist = float('inf')\n\n            for pkg in env.packages:\n                if pkg.status == 'waiting' and pkg.start_time <= env.t:\n                    dist = manhattan_distance(pos[:2], pkg.start)\n                    if dist<closest_dist:\n                        closest_dist = dist\n                        closest_pkg = pkg\n                        \n            if closest_pkg:\n                old_dist = manhattan_distance(robot_positions_before[i][:2], closest_pkg.start)\n                new_dist = manhattan_distance(pos[:2], closest_pkg.start)\n                if new_dist < old_dist:\n                    shaped_reward += 0.2 * (old_dist - new_dist) / (old_dist + 1)\n\n    # Urgency rewards - prioritize packages close to deadline\n    for package in env.packages:\n        if package.status == 'delivered':\n            if env.t <= package.deadline:\n                shaped_reward += 1.0\n        elif package.status == 'waiting' and package.start_time <= env.t:\n            time_left = package.deadline - env.t\n            if time_left <= 5:  # Critical deadline\n                # Find closest robot\n                closest_dist = float('inf')\n                for pos in robot_positions_after:\n                    if pos[2] == 0:  # Only consider robots not carrying anything\n                        dist = manhattan_distance(pos[:2], package.start)\n                        closest_dist = min(closest_dist, dist)\n\n                if closest_dist < float('inf'):\n                    urgency_factor = max(0, (5 - time_left)) / 5\n                    shaped_reward += 0.3 * urgency_factor / (closest_dist + 1)\n\n    # Time pressure for waiting packages\n    waiting_packages = sum(1 for p in env.packages if p.status == 'waiting' and p.start_time <= env.t)\n    if waiting_packages > 0:\n        shaped_reward -= 0.005 * waiting_packages\n\n    return shaped_reward\n\ndef manhattan_distance(pos1, pos2):\n    \"\"\"Calculate Manhattan distance between two positions\"\"\"\n    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:01:52.759411Z","iopub.execute_input":"2025-05-13T02:01:52.759774Z","iopub.status.idle":"2025-05-13T02:01:52.777827Z","shell.execute_reply.started":"2025-05-13T02:01:52.759747Z","shell.execute_reply":"2025-05-13T02:01:52.776954Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom collections import deque, namedtuple\nimport random\n\n# Experience tuple for replay buffer\nExperience = namedtuple('Experience',\n                        ['state', 'action', 'reward', 'next_state', 'done', 'agent_id'])\n\nclass ReplayBuffer:\n    \"\"\"Shared replay buffer for experience replay across agents\"\"\"\n    def __init__(self, capacity=5000):\n        self.buffer = deque(maxlen=capacity)\n        \n    def add(self, state, action, reward, next_state, done, agent_id):\n        self.buffer.append(Experience(state, action, reward, next_state, done, agent_id))\n\n    def sample(self, batch_size):\n        experiences = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n\n        states = [e.state for e in experiences]\n        actions = [e.action for e in experiences]\n        rewards = torch.tensor([e.reward for e in experiences], dtype=torch.float)\n        next_states = [e.next_state for e in experiences]\n        dones = torch.tensor([e.done for e in experiences], dtype=torch.float)\n        agent_ids = [e.agent_id for e in experiences]\n\n        return states, actions, rewards, next_states, dones, agent_ids\n\n    def __len__(self):\n        return len(self.buffer)\n\nclass ActorNetwork(nn.Module):\n    \"\"\"Actor network for policy approximation\"\"\"\n    def __init__(self, state_size, action_size, hidden_size=64):\n        super(ActorNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n\n        # Separate outputs for movement and package actions\n        self.move_head = nn.Linear(hidden_size, 5)  # 5 movement actions: S, L, R, U, D\n        self.pkg_head = nn.Linear(hidden_size, 3)   # 3 package actions: 0, 1, 2\n\n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n\n        move_probs = F.softmax(self.move_head(x), dim=-1)\n        pkg_probs = F.softmax(self.pkg_head(x), dim=-1)\n\n        return move_probs, pkg_probs\n\nclass CriticNetwork(nn.Module):\n    \"\"\"Critic network for value function approximation\"\"\"\n    def __init__(self, state_size, hidden_size=64):\n        super(CriticNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n\n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass Policy:\n    \"\"\"Individual agent using SEAC algorithm\"\"\"\n    def __init__(self, state_size, action_size, agent_id, shared_buffer,\n                 learning_rate=3e-4, gamma=0.99):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.agent_id = agent_id\n        self.shared_buffer = shared_buffer\n        self.gamma = gamma\n\n        # Networks\n        self.actor = ActorNetwork(state_size, action_size)\n        self.critic = CriticNetwork(state_size)\n\n        # Target networks (for stability)\n        self.target_critic = CriticNetwork(state_size)\n        self.target_critic.load_state_dict(self.critic.state_dict())\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)\n\n        # For exploration\n        self.eps = 0.1\n        self.move_actions = ['S', 'L', 'R', 'U', 'D']\n        self.pkg_actions = ['0', '1', '2']\n\n        # Learning parameters\n        self.tau = 0.01  # For soft target updates\n\n    def get_action(self, state_tensor, explore=True):\n        if state_tensor.shape[0] != self.state_size:\n          diff = self.state_size - state_tensor.shape[0]\n          if diff > 0:\n            padding = torch.zeros(diff, dtype=state_tensor.dtype, device=state_tensor.device)\n            state_tensor = torch.cat([state_tensor, padding])\n          else:\n            # print('Truncate state tensor')\n            state_tensor = state_tensor[:self.state_tensor]\n        with torch.no_grad():\n            move_probs, pkg_probs = self.actor(state_tensor)\n\n        # Epsilon-greedy exploration\n        if explore and random.random() < self.eps:\n            move_idx = random.randint(0, 4)  # Random movement action\n            pkg_idx = random.randint(0, 2)   # Random package action\n        else:\n            move_idx = torch.argmax(move_probs).item()\n            pkg_idx = torch.argmax(pkg_probs).item()\n\n        return (self.move_actions[move_idx], self.pkg_actions[pkg_idx])\n\n    def learn(self, robots_before, packages_before, batch_size=64):\n        if len(self.shared_buffer) < batch_size:\n            return\n\n        states, actions, rewards, next_states, dones, _ = self.shared_buffer.sample(batch_size)\n        \n        # Convert states to tensors\n        state_tensors = torch.stack([self._preprocess_state(s) for s in states])\n        next_state_tensors = torch.stack([self._preprocess_state(s) for s in next_states])\n\n        # Compute target values (TD target) using target network for stability\n        with torch.no_grad():\n            next_values = self.target_critic(next_state_tensors).squeeze()\n            target_values = rewards + self.gamma * next_values * (1 - dones)\n\n        # Compute current value estimates\n        values = self.critic(state_tensors).squeeze()\n\n        # Critic loss (MSE)\n        critic_loss = F.mse_loss(values, target_values)\n\n        # Actor loss\n        # We need to process each experience individually for the actor loss\n        actor_loss = 0\n        entropy_loss = 0  # Add entropy to encourage exploration\n        for i, (state, action) in enumerate(zip(state_tensors, actions)):\n            move_action, pkg_action = action\n            move_idx = self.move_actions.index(move_action)\n            pkg_idx = int(pkg_action)\n\n            move_probs, pkg_probs = self.actor(state)\n            advantage = target_values[i] - values[i].detach()\n\n            # Log probability of the taken actions\n            log_prob_move = torch.log(move_probs[move_idx] + 1e-10)\n            log_prob_pkg = torch.log(pkg_probs[pkg_idx] + 1e-10)\n\n            # Policy gradient loss\n            actor_loss -= (log_prob_move + log_prob_pkg) * advantage\n\n            # Entropy loss to encourage exploration\n            entropy_move = -torch.sum(move_probs * torch.log(move_probs + 1e-10))\n            entropy_pkg = -torch.sum(pkg_probs * torch.log(pkg_probs + 1e-10))\n            entropy_loss -= 0.01 * (entropy_move + entropy_pkg)  # Small entropy coefficient\n\n        actor_loss = (actor_loss + entropy_loss) / len(states)\n\n        # Update networks\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)  # Gradient clipping\n        self.critic_optimizer.step()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)  # Gradient clipping\n        self.actor_optimizer.step()\n\n        # Soft update target networks\n        self._soft_update(self.critic, self.target_critic)\n\n    def _soft_update(self, source, target):\n        \"\"\"Soft update: target = tau*source + (1 - tau)*target\"\"\"\n        for target_param, source_param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(\n                self.tau * source_param.data + (1.0 - self.tau) * target_param.data\n            )\n\n    def _preprocess_state(self, state):\n        \"\"\"Convert environment state to tensor representation\"\"\"\n        # Extract relevant information from state\n        time_step = state['time_step']\n        grid = state['map']\n        robots = state['robots']\n        packages = state.get('packages', [])\n\n        # Flatten grid for simplicity\n        flat_grid = [item for row in grid for item in row]\n\n        # Extract robot features\n        robot_info = []\n        my_robot = None\n\n        for robot_idx, (r, c, carrying) in enumerate(robots):\n            if robot_idx == self.agent_id:\n                my_robot = (r-1, c-1, carrying)  # Convert back to 0-indexed\n            else:\n                robot_info.extend([r-1, c-1, carrying])  # Convert back to 0-indexed\n\n        # Ensure my_robot is at the beginning of the feature vector\n        if my_robot:\n            robot_info = [my_robot[0], my_robot[1], my_robot[2]] + robot_info\n\n        # Extract package features - focus on active packages\n        package_info = []\n        for pkg_id, start_r, start_c, target_r, target_c, start_time, deadline in packages:\n            # Only consider packages that are active now\n            if start_time <= time_step:\n                package_info.extend([start_r-1, start_c-1, target_r-1, target_c-1,\n                                    deadline - time_step])  # Convert back to 0-indexed\n\n        max_packages = PACKAGES\n        while len(package_info) < max_packages * 5:\n            package_info.extend([-1, -1, -1, -1, -1])  # Padding for non-existent packages\n\n        # Combine all features\n        features = [time_step] + robot_info + package_info\n\n        return torch.tensor(features, dtype=torch.float)\n\nclass Agents:\n    \"\"\"Main SEAC algorithm implementation with shared experience\"\"\"\n    def __init__(self):\n        self.agents = []\n        self.n_robots = 0\n        self.state = None\n        self.shared_buffer = ReplayBuffer(capacity=5000)  # Increased buffer size\n        self.state_size = None  # Will be determined during initialization\n        self.action_size = 8  # 5 movement actions * 3 package actions\n        self.batch_size = 128  # Increased batch size for more stable learning\n        self.update_frequency = 5  # Update networks every N steps\n        self.step_counter = 0\n        self.prev_states = {}\n        self.prev_actions = {}\n        self.prev_rewards = {}  # Track previous rewards for n-step returns\n        self.training_mode = True  # Set to False for evaluation only\n        self.training_stats = {\n            'critic_losses': [],\n            'actor_losses': [],\n            'avg_values': [],\n            'avg_advantages': []\n        }  # For tracking training progress\n        \n\n    def init_agents(self, state):\n        \"\"\"Initialize SEAC agents\"\"\"\n        self.state = state\n        self.n_robots = len(state['robots'])\n        self.map = state['map']\n\n        # Calculate state size based on preprocessed state\n        # This includes time step, all robot positions and carrying status, and package info\n        # We'll create a dummy agent to get the state size\n        dummy_agent = Policy(0, self.action_size, 0, self.shared_buffer)\n        dummy_state_tensor = dummy_agent._preprocess_state(state)\n        self.state_size = len(dummy_state_tensor)\n\n        # Create agents with correct state size\n        self.agents = [Policy(self.state_size, self.action_size, i, self.shared_buffer)\n                      for i in range(self.n_robots)]\n\n        # Initialize previous states and actions\n        for i in range(self.n_robots):\n            self.prev_states[i] = None\n            self.prev_actions[i] = None\n\n    def get_actions(self, state):\n        \"\"\"Determine actions for all agents\"\"\"\n        self.step_counter += 1\n        actions = []\n        robots_before = state['robots']\n        packages_before = state['packages']\n        \n        for i, agent in enumerate(self.agents):\n            # Preprocess state for the current agent\n            state_tensor = agent._preprocess_state(state)\n\n            # Get action from the agent's policy\n            action = agent.get_action(state_tensor, explore=self.training_mode)\n            actions.append(action)\n\n            # Store experience if we have a previous state and action\n            if self.training_mode and self.prev_states[i] is not None:\n                # Get reward from environment (shared total reward for simplicity)\n                # In a more sophisticated implementation, you might want to implement reward shaping\n                # to better attribute rewards to individual agents\n                reward = state.get('reward', 0)\n                done = state.get('done', False)\n\n                # Add experience to shared buffer\n                self.shared_buffer.add(\n                    self.prev_states[i],\n                    self.prev_actions[i],\n                    reward,\n                    state,\n                    done,\n                    i\n                )\n\n            # Update previous state and action\n            self.prev_states[i] = state\n            self.prev_actions[i] = action\n\n        # Update networks periodically\n        if self.training_mode and self.step_counter % self.update_frequency == 0:\n            for agent in self.agents:\n                agent.learn(robots_before, packages_before, self.batch_size)\n                \n        return actions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:01:52.879177Z","iopub.execute_input":"2025-05-13T02:01:52.879470Z","iopub.status.idle":"2025-05-13T02:01:52.920377Z","shell.execute_reply.started":"2025-05-13T02:01:52.879450Z","shell.execute_reply":"2025-05-13T02:01:52.919356Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def train(env, agents, num_episodes=1000, print_freq=10, save_path=None,\n          initial_eps=0.3, final_eps=0.01, eps_decay_episodes=800):\n    \"\"\"\n    Train the SEAC agents over multiple episodes\n\n    Args:\n        env: Environment to train in\n        agents: SEAC agents\n        num_episodes: Total number of training episodes\n        print_freq: How often to print progress\n        save_path: Where to save model checkpoints (None = don't save)\n        initial_eps: Starting exploration rate\n        final_eps: Final exploration rate\n        eps_decay_episodes: Over how many episodes to decay epsilon\n\n    Returns:\n        list: Episode rewards\n    \"\"\"\n    episode_rewards = []\n    best_avg_reward = float('-inf')\n    original_rewards = []\n    \n    # Set initial epsilon for all agents\n    for agent in agents.agents:\n        agent.eps = initial_eps\n\n    for episode in range(1, num_episodes + 1):\n        state = env.reset()\n        agents.init_agents(state)\n\n        # Reset experience tracking for new episode\n        for i in range(agents.n_robots):\n            agents.prev_states[i] = None\n            agents.prev_actions[i] = None\n\n        # Decay epsilon\n        if episode <= eps_decay_episodes:\n            current_eps = initial_eps - (initial_eps - final_eps) * (episode / eps_decay_episodes)\n            for agent in agents.agents:\n                agent.eps = current_eps\n\n        # Run episode\n        done = False\n        episode_reward = 0\n        episode_org_reward = 0\n        t = 0\n\n        while not done:\n            actions = agents.get_actions(state)\n            next_state, reward, done, infos = env.step(actions)\n            shaped_reward = reward_shaping(reward, env, next_state, state, actions)\n            # print(f\"shaped reward {shaped_reward}\")\n            # Add reward to state for experience collection\n            next_state['reward'] = shaped_reward\n            next_state['done'] = done\n\n            # Update state for next iteration\n            state = next_state\n            episode_reward += shaped_reward\n            episode_org_reward += reward\n            t += 1\n            \n            if t == 1000:\n                break\n\n        successful_deliveries = sum(1 for package in env.packages if package.status == 'delivered')\n                \n        original_rewards.append(episode_org_reward)\n        episode_rewards.append(episode_reward)\n\n        # Print progress\n        if episode % print_freq == 0:\n            avg_reward = sum(episode_rewards[-print_freq:]) / print_freq\n            avg_org_reward = sum(original_rewards[-print_freq:]) / print_freq\n            print(f\"Episode {episode}/{num_episodes}, \"f\"Avg Shaped Reward: {avg_reward:.2f}, \"f\"Avg Original Reward: {avg_org_reward:.2f}, \"f\"Epsilon: {agents.agents[0].eps:.3f}, \"f\"Buffer Size: {len(agents.shared_buffer)}\")\n        if wandb.run is not None:\n            wandb.log({\n                \"episode\": episode,\n                \"reward per episode\": episode_reward,\n                \"original reward per episode\": episode_org_reward,\n                \"epsilon\": agents.agents[0].eps,\n                \"buffer\": len(agents.shared_buffer),\n                \"successful_deliveries per episode\": successful_deliveries\n            })\n\n    return episode_rewards\n\n\ndef evaluate(env, agents, num_episodes=10):\n    \"\"\"\n    Evaluate trained agents without exploration\n\n    Args:\n        env: Environment to evaluate in\n        agents: Trained SEAC agents\n        num_episodes: Number of evaluation episodes\n\n    Returns:\n        float: Average reward per episode\n    \"\"\"\n    # Disable exploration and training\n    training_mode_backup = agents.training_mode\n    agents.training_mode = False\n\n    total_rewards = []\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        episode_reward = 0\n\n        while not done:\n            actions = agents.get_actions(state)\n            next_state, reward, done, _ = env.step(actions)\n\n            state = next_state\n            episode_reward += reward\n\n        total_rewards.append(episode_reward)\n        print(f\"Evaluation Episode {episode+1}/{num_episodes}, Reward: {episode_reward:.2f}\")\n\n    # Restore training mode\n    agents.training_mode = training_mode_backup\n\n    avg_reward = sum(total_rewards) / num_episodes\n    print(f\"Evaluation complete. Average Reward: {avg_reward:.2f}\")\n    return avg_reward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:12:53.389111Z","iopub.execute_input":"2025-05-13T02:12:53.389811Z","iopub.status.idle":"2025-05-13T02:12:53.407067Z","shell.execute_reply.started":"2025-05-13T02:12:53.389622Z","shell.execute_reply":"2025-05-13T02:12:53.405952Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"maps = {\n    \"map1\": {\"map_file\": 'map1.txt',\"max_time_steps\": 1000,'n_robots': 5, \"n_packages\": 100, 'seed': 10},\n    \"map2\": {\"map_file\": 'map2.txt',\"max_time_steps\": 1000,'n_robots': 5, \"n_packages\": 100, 'seed': 10},\n    \"map3\": {\"map_file\": 'map3.txt',\"max_time_steps\": 1000,'n_robots': 5, \"n_packages\": 500, 'seed': 10},\n    \"map4\": {\"map_file\": 'map4.txt',\"max_time_steps\": 1000,'n_robots': 10, \"n_packages\": 500, 'seed': 10},\n    \"map5\": {\"map_file\": 'map5.txt',\"max_time_steps\": 1000,'n_robots': 10, \"n_packages\": 100, 'seed': 10},\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Import necessary modules\nfrom env import Environment\nimport os\nimport matplotlib.pyplot as plt\n\n\n# Create environment\nselected = 'map1'\nif selected == 'map1':\n    PATH = maps[selected]['map_file']\n    ROBOTS = maps[selected]['n_robot']\n    PACKAGES = maps[selected]['n_packages']\n    MAX_TIME_STEPS = 1000\n    SEED = 10\nelse:\n    print('Using map1 config')\n    PATH = 'map1.txt'\n    ROBOTS = 5\n    PACKAGES = 100\n    MAX_TIME_STEPS = 1000\n    SEED = 10\n\nenv = Environment(map_file=PATH, max_time_steps=MAX_TIME_STEPS, n_robots=ROBOTS, n_packages=PACKAGES, seed=SEED)\n\n# Initialize agents\nagents = Agents()\nstate = env.reset()\nagents.init_agents(state)\n\n# Train agents\nwandb.init(project=\"MARL-pickup-and-delivery\", name=\"SEAC\", config={\n    \"num_episodes\": 1000,\n    \"initial_eps\": 0.3,\n    \"final_eps\": 0.01,\n    \"eps_decay_episodes\": 800\n})\n\nprint(\"Starting training...\")\nepisode_rewards = train(\n    env,\n    agents,\n    num_episodes=250,\n    print_freq=10,\n    initial_eps=0.3,\n    final_eps=0.01,\n    eps_decay_episodes=250\n)\n\n# Plot training progress\nplt.figure(figsize=(10, 5))\nplt.plot(episode_rewards)\nplt.title('SEAC Training Progress')\nplt.xlabel('Episode')\nplt.ylabel('Episode Reward')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:12:56.994991Z","iopub.execute_input":"2025-05-13T02:12:56.995487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/marl-delivery/wandb/run-20250513_021257-999c91vt</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/999c91vt' target=\"_blank\">SEAC</a></strong> to <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery' target=\"_blank\">https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/999c91vt' target=\"_blank\">https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/999c91vt</a>"},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n","output_type":"stream"},{"name":"stdout","text":"Episode 10/250, Avg Shaped Reward: 13533.63, Avg Original Reward: -4.46, Epsilon: 0.288, Buffer Size: 5000\nEpisode 20/250, Avg Shaped Reward: 5494.23, Avg Original Reward: -8.04, Epsilon: 0.277, Buffer Size: 5000\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"agents.training_mode = False\nfor agent in agents.agents:\n    agent.eps = 0.0\nfor agent in agents.agents:\n    agent.actor.eval()\ndone = False\nstate = env.reset()\nt = 0\nreward_history = []\nwhile not done:\n    actions = agents.get_actions(state) \n    state, reward, done, infos = env.step(actions)\n\n    print(\"\\nState after step:\")\n    env.render()\n    print(f\"Reward: {reward}, Done: {done}, Infos: {infos}\")\n    print(\"Time step:\", env.t)\n    print(\"Packages:\", state['packages'])\n    print(\"Robots:\", state['robots'])\n    reward_history.append(env.total_reward)\n    # For debug purpose\n    t += 1\n    if t == 1000:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:12:32.567775Z","iopub.status.idle":"2025-05-13T02:12:32.568112Z","shell.execute_reply.started":"2025-05-13T02:12:32.567972Z","shell.execute_reply":"2025-05-13T02:12:32.567986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(reward_history, label=\"Total Reward\")\nplt.xlabel(\"Time Step\")\nplt.ylabel(\"Total Reward\")\nplt.title(\"Total Reward over Time\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T02:11:11.976772Z","iopub.status.idle":"2025-05-13T02:11:11.977203Z","shell.execute_reply.started":"2025-05-13T02:11:11.977027Z","shell.execute_reply":"2025-05-13T02:11:11.977049Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}