{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!git clone https://github.com/cuongtv312/marl-delivery.git\n%cd marl-delivery\n!pip install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:54:34.183651Z","iopub.execute_input":"2025-05-21T11:54:34.183940Z","iopub.status.idle":"2025-05-21T11:54:40.989922Z","shell.execute_reply.started":"2025-05-21T11:54:34.183912Z","shell.execute_reply":"2025-05-21T11:54:40.988574Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from env import Environment\nimport gymnasium as gym\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:54:40.992332Z","iopub.execute_input":"2025-05-21T11:54:40.992747Z","iopub.status.idle":"2025-05-21T11:54:42.095191Z","shell.execute_reply.started":"2025-05-21T11:54:40.992704Z","shell.execute_reply":"2025-05-21T11:54:42.093840Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:596: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n    fn()\n  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 304, in register_gymnasium_envs\n    _register_atari_envs()\n  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 205, in _register_atari_envs\n    import ale_py\n  File \"/usr/local/lib/python3.11/dist-packages/ale_py/__init__.py\", line 68, in <module>\n    register_v0_v4_envs()\n  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 178, in register_v0_v4_envs\n    _register_rom_configs(legacy_games, obs_types, versions)\n  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 63, in _register_rom_configs\n    gymnasium.register(\n    ^^^^^^^^^^^^^^^^^^\nAttributeError: partially initialized module 'gymnasium' has no attribute 'register' (most likely due to a circular import)\n\u001b[0m\n  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=wandb_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:57:25.942159Z","iopub.execute_input":"2025-05-21T11:57:25.942534Z","iopub.status.idle":"2025-05-21T11:57:35.398438Z","shell.execute_reply.started":"2025-05-21T11:57:25.942509Z","shell.execute_reply":"2025-05-21T11:57:35.397364Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/notebook/utils.py:280: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  return LooseVersion(v) >= LooseVersion(check)\n/usr/local/lib/python3.11/dist-packages/google/colab/_import_hooks/_altair.py:16: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n  import imp  # pylint: disable=deprecated-module\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpage0526\u001b[0m (\u001b[33miai-uet-vnu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"def reward_shaping(reward, env, next_state, state, actions):\n    shaped_reward = reward\n    robots_before = state['robots']\n    robots_after = next_state['robots']\n\n    # Track newly picked up packages\n    packages_carried_before = [r[2] for r in robots_before]\n    packages_carried_after = [r[2] for r in robots_after]\n    newly_picked_up = [p for p in packages_carried_after if p > 0 and p not in packages_carried_before]\n    # Reward for each newly picked up package\n    shaped_reward += 0.5 * len(newly_picked_up)\n\n    # Convert robot positions to 0-indexed (environment uses 0-indexed internally)\n    robot_positions_before = [(r[0]-1, r[1]-1, r[2]) for r in robots_before]\n    robot_positions_after = [(r[0]-1, r[1]-1, r[2]) for r in robots_after]\n    # Progress rewards for robots carrying packages\n    for i, (pos_before, pos_after) in enumerate(zip(robot_positions_before, robot_positions_after)):\n        pkg_id = pos_after[2]\n        # Skip if not carrying anything\n        if pkg_id == 0:\n            continue\n        # Find package target\n        package = next((p for p in env.packages if p.package_id == pkg_id), None)\n        if package:\n            # If robot moved while carrying package\n            if pos_before[:2] != pos_after[:2]:\n                # Calculate distances to target\n                old_dist = manhattan_distance(pos_before[:2], package.target)\n                new_dist = manhattan_distance(pos_after[:2], package.target)\n                # Reward progress toward target\n                if new_dist < old_dist:\n                    progress = (old_dist - new_dist) / (old_dist + 0.001)  # Normalized progress\n                    shaped_reward += 0.1 * progress\n                elif new_dist > old_dist:\n                    shaped_reward -= 0.05  # Small penalty for moving away\n                    \n    # Urgency rewards - prioritize packages close to deadline\n    for package in env.packages:\n        if package.status == 'delivered':\n            if env.t <= package.deadline:\n                shaped_reward += 1.0\n        elif package.status == 'waiting' and package.start_time <= env.t:\n            time_left = package.deadline - env.t\n            if time_left <= 5:  # Critical deadline\n                # Find closest robot\n                closest_dist = float('inf')\n                for pos in robot_positions_after:\n                    if pos[2] == 0:  # Only consider robots not carrying anything\n                        dist = manhattan_distance(pos[:2], package.start)\n                        closest_dist = min(closest_dist, dist)\n                if closest_dist < float('inf'):\n                    urgency_factor = max(0, (5 - time_left)) / 5\n                    shaped_reward += 0.2 * urgency_factor / (closest_dist + 1)\n\n    # Time pressure for waiting packages\n    waiting_packages = sum(1 for p in env.packages if p.status == 'waiting' and p.start_time <= env.t)\n    if waiting_packages > 0:\n        shaped_reward -= 0.01 * waiting_packages\n    return shaped_reward\n    \ndef manhattan_distance(pos1, pos2):\n    \"\"\"Calculate Manhattan distance between two positions\"\"\"\n    return abs(pos1[0] - pos2[0]) + abs(pos1[1] - pos2[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:57:35.400383Z","iopub.execute_input":"2025-05-21T11:57:35.401066Z","iopub.status.idle":"2025-05-21T11:57:35.416555Z","shell.execute_reply.started":"2025-05-21T11:57:35.401040Z","shell.execute_reply":"2025-05-21T11:57:35.414709Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom collections import deque, namedtuple\nimport random\n\n# Experience tuple for replay buffer\nExperience = namedtuple('Experience',\n                        ['state', 'action', 'reward', 'next_state', 'done', 'agent_id'])\n\nclass ReplayBuffer:\n    \"\"\"Shared replay buffer for experience replay across agents\"\"\"\n    def __init__(self, capacity=5000):\n        self.buffer = deque(maxlen=capacity)\n        \n    def add(self, state, action, reward, next_state, done, agent_id):\n        self.buffer.append(Experience(state, action, reward, next_state, done, agent_id))\n\n    def sample(self, batch_size):\n        experiences = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n\n        states = [e.state for e in experiences]\n        actions = [e.action for e in experiences]\n        rewards = torch.tensor([e.reward for e in experiences], dtype=torch.float)\n        next_states = [e.next_state for e in experiences]\n        dones = torch.tensor([e.done for e in experiences], dtype=torch.float)\n        agent_ids = [e.agent_id for e in experiences]\n\n        return states, actions, rewards, next_states, dones, agent_ids\n\n    def __len__(self):\n        return len(self.buffer)\n\nclass ActorNetwork(nn.Module):\n    \"\"\"Actor network for policy approximation\"\"\"\n    def __init__(self, state_size, action_size, hidden_size=64):\n        super(ActorNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n\n        # Separate outputs for movement and package actions\n        self.move_head = nn.Linear(hidden_size, 5)  # 5 movement actions: S, L, R, U, D\n        self.pkg_head = nn.Linear(hidden_size, 3)   # 3 package actions: 0, 1, 2\n\n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n\n        move_probs = F.softmax(self.move_head(x), dim=-1)\n        pkg_probs = F.softmax(self.pkg_head(x), dim=-1)\n\n        return move_probs, pkg_probs\n\nclass CriticNetwork(nn.Module):\n    \"\"\"Critic network for value function approximation\"\"\"\n    def __init__(self, state_size, hidden_size=64):\n        super(CriticNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n\n    def forward(self, state):\n        x = F.relu(self.fc1(state))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n\nclass Policy:\n    \"\"\"Individual agent using SEAC algorithm\"\"\"\n    def __init__(self, state_size, action_size, agent_id, shared_buffer,\n                 learning_rate=3e-4, gamma=0.99):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.agent_id = agent_id\n        self.shared_buffer = shared_buffer\n        self.gamma = gamma\n\n        # Networks\n        self.actor = ActorNetwork(state_size, action_size)\n        self.critic = CriticNetwork(state_size)\n\n        # Target networks (for stability)\n        self.target_critic = CriticNetwork(state_size)\n        self.target_critic.load_state_dict(self.critic.state_dict())\n\n        # Optimizers\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=learning_rate)\n\n        # For exploration\n        self.eps = 0.1\n        self.move_actions = ['S', 'L', 'R', 'U', 'D']\n        self.pkg_actions = ['0', '1', '2']\n\n        # Learning parameters\n        self.tau = 0.01  # For soft target updates\n\n    def get_action(self, state_tensor, explore=True):\n        if state_tensor.shape[0] != self.state_size:\n          diff = self.state_size - state_tensor.shape[0]\n          if diff > 0:\n            padding = torch.zeros(diff, dtype=state_tensor.dtype, device=state_tensor.device)\n            state_tensor = torch.cat([state_tensor, padding])\n          else:\n            # print('Truncate state tensor')\n            state_tensor = state_tensor[:self.state_tensor]\n        with torch.no_grad():\n            move_probs, pkg_probs = self.actor(state_tensor)\n\n        # Epsilon-greedy exploration\n        if explore and random.random() < self.eps:\n            move_idx = random.randint(0, 4)  # Random movement action\n            pkg_idx = random.randint(0, 2)   # Random package action\n        else:\n            move_idx = torch.argmax(move_probs).item()\n            pkg_idx = torch.argmax(pkg_probs).item()\n\n        return (self.move_actions[move_idx], self.pkg_actions[pkg_idx])\n\n    def learn(self, robots_before, packages_before, batch_size=64):\n        if len(self.shared_buffer) < batch_size:\n            return\n\n        states, actions, rewards, next_states, dones, _ = self.shared_buffer.sample(batch_size)\n        \n        # Convert states to tensors\n        state_tensors = torch.stack([self._preprocess_state(s) for s in states])\n        next_state_tensors = torch.stack([self._preprocess_state(s) for s in next_states])\n\n        # Compute target values (TD target) using target network for stability\n        with torch.no_grad():\n            next_values = self.target_critic(next_state_tensors).squeeze()\n            target_values = rewards + self.gamma * next_values * (1 - dones)\n\n        # Compute current value estimates\n        values = self.critic(state_tensors).squeeze()\n\n        # Critic loss (MSE)\n        critic_loss = F.mse_loss(values, target_values)\n\n        # Actor loss\n        # We need to process each experience individually for the actor loss\n        actor_loss = 0\n        entropy_loss = 0  # Add entropy to encourage exploration\n        for i, (state, action) in enumerate(zip(state_tensors, actions)):\n            move_action, pkg_action = action\n            move_idx = self.move_actions.index(move_action)\n            pkg_idx = int(pkg_action)\n\n            move_probs, pkg_probs = self.actor(state)\n            advantage = target_values[i] - values[i].detach()\n\n            # Log probability of the taken actions\n            log_prob_move = torch.log(move_probs[move_idx] + 1e-10)\n            log_prob_pkg = torch.log(pkg_probs[pkg_idx] + 1e-10)\n\n            # Policy gradient loss\n            actor_loss -= (log_prob_move + log_prob_pkg) * advantage\n\n            # Entropy loss to encourage exploration\n            entropy_move = -torch.sum(move_probs * torch.log(move_probs + 1e-10))\n            entropy_pkg = -torch.sum(pkg_probs * torch.log(pkg_probs + 1e-10))\n            entropy_loss -= 0.01 * (entropy_move + entropy_pkg)  # Small entropy coefficient\n\n        actor_loss = (actor_loss + entropy_loss) / len(states)\n\n        # Update networks\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)  # Gradient clipping\n        self.critic_optimizer.step()\n\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)  # Gradient clipping\n        self.actor_optimizer.step()\n\n        # Soft update target networks\n        self._soft_update(self.critic, self.target_critic)\n\n    def _soft_update(self, source, target):\n        \"\"\"Soft update: target = tau*source + (1 - tau)*target\"\"\"\n        for target_param, source_param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(\n                self.tau * source_param.data + (1.0 - self.tau) * target_param.data\n            )\n\n    def _preprocess_state(self, state):\n        \"\"\"Convert environment state to tensor representation\"\"\"\n        # Extract relevant information from state\n        time_step = state['time_step']\n        grid = state['map']\n        robots = state['robots']\n        packages = state.get('packages', [])\n\n        # Flatten grid for simplicity\n        flat_grid = [item for row in grid for item in row]\n\n        # Extract robot features\n        robot_info = []\n        my_robot = None\n\n        for robot_idx, (r, c, carrying) in enumerate(robots):\n            if robot_idx == self.agent_id:\n                my_robot = (r-1, c-1, carrying)  # Convert back to 0-indexed\n            else:\n                robot_info.extend([r-1, c-1, carrying])  # Convert back to 0-indexed\n\n        # Ensure my_robot is at the beginning of the feature vector\n        if my_robot:\n            robot_info = [my_robot[0], my_robot[1], my_robot[2]] + robot_info\n\n        # Extract package features - focus on active packages\n        package_info = []\n        for pkg_id, start_r, start_c, target_r, target_c, start_time, deadline in packages:\n            # Only consider packages that are active now\n            if start_time <= time_step:\n                package_info.extend([start_r-1, start_c-1, target_r-1, target_c-1,\n                                    deadline - time_step])  # Convert back to 0-indexed\n\n        max_packages = PACKAGES\n        while len(package_info) < max_packages * 5:\n            package_info.extend([-1, -1, -1, -1, -1])  # Padding for non-existent packages\n\n        # Combine all features\n        features = [time_step] + robot_info + package_info\n\n        return torch.tensor(features, dtype=torch.float)\n\nclass Agents:\n    \"\"\"Main SEAC algorithm implementation with shared experience\"\"\"\n    def __init__(self):\n        self.agents = []\n        self.n_robots = 0\n        self.state = None\n        self.shared_buffer = ReplayBuffer(capacity=5000)  # Increased buffer size\n        self.state_size = None  # Will be determined during initialization\n        self.action_size = 8  # 5 movement actions * 3 package actions\n        self.batch_size = 128  # Increased batch size for more stable learning\n        self.update_frequency = 5  # Update networks every N steps\n        self.step_counter = 0\n        self.prev_states = {}\n        self.prev_actions = {}\n        self.prev_rewards = {}  # Track previous rewards for n-step returns\n        self.training_mode = True  # Set to False for evaluation only\n        self.training_stats = {\n            'critic_losses': [],\n            'actor_losses': [],\n            'avg_values': [],\n            'avg_advantages': []\n        }  # For tracking training progress\n        \n\n    def init_agents(self, state):\n        \"\"\"Initialize SEAC agents\"\"\"\n        self.state = state\n        self.n_robots = len(state['robots'])\n        self.map = state['map']\n\n        # Calculate state size based on preprocessed state\n        # This includes time step, all robot positions and carrying status, and package info\n        # We'll create a dummy agent to get the state size\n        dummy_agent = Policy(0, self.action_size, 0, self.shared_buffer)\n        dummy_state_tensor = dummy_agent._preprocess_state(state)\n        self.state_size = len(dummy_state_tensor)\n\n        # Create agents with correct state size\n        self.agents = [Policy(self.state_size, self.action_size, i, self.shared_buffer)\n                      for i in range(self.n_robots)]\n\n        # Initialize previous states and actions\n        for i in range(self.n_robots):\n            self.prev_states[i] = None\n            self.prev_actions[i] = None\n\n    def get_actions(self, state):\n        \"\"\"Determine actions for all agents\"\"\"\n        self.step_counter += 1\n        actions = []\n        robots_before = state['robots']\n        packages_before = state['packages']\n        \n        for i, agent in enumerate(self.agents):\n            # Preprocess state for the current agent\n            state_tensor = agent._preprocess_state(state)\n\n            # Get action from the agent's policy\n            action = agent.get_action(state_tensor, explore=self.training_mode)\n            actions.append(action)\n\n            # Store experience if we have a previous state and action\n            if self.training_mode and self.prev_states[i] is not None:\n                # Get reward from environment (shared total reward for simplicity)\n                # In a more sophisticated implementation, you might want to implement reward shaping\n                # to better attribute rewards to individual agents\n                reward = state.get('reward', 0)\n                done = state.get('done', False)\n\n                # Add experience to shared buffer\n                self.shared_buffer.add(\n                    self.prev_states[i],\n                    self.prev_actions[i],\n                    reward,\n                    state,\n                    done,\n                    i\n                )\n\n            # Update previous state and action\n            self.prev_states[i] = state\n            self.prev_actions[i] = action\n\n        # Update networks periodically\n        if self.training_mode and self.step_counter % self.update_frequency == 0:\n            for agent in self.agents:\n                agent.learn(robots_before, packages_before, self.batch_size)\n                \n        return actions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:57:35.418478Z","iopub.execute_input":"2025-05-21T11:57:35.418920Z","iopub.status.idle":"2025-05-21T11:57:41.151112Z","shell.execute_reply.started":"2025-05-21T11:57:35.418885Z","shell.execute_reply":"2025-05-21T11:57:41.150047Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def train(env, agents, num_episodes=1000, print_freq=10, save_path=None,\n          initial_eps=0.3, final_eps=0.01, eps_decay_episodes=800):\n    \"\"\"\n    Train the SEAC agents over multiple episodes\n\n    Args:\n        env: Environment to train in\n        agents: SEAC agents\n        num_episodes: Total number of training episodes\n        print_freq: How often to print progress\n        save_path: Where to save model checkpoints (None = don't save)\n        initial_eps: Starting exploration rate\n        final_eps: Final exploration rate\n        eps_decay_episodes: Over how many episodes to decay epsilon\n\n    Returns:\n        list: Episode rewards\n    \"\"\"\n    episode_rewards = []\n    best_avg_reward = float('-inf')\n    original_rewards = []\n    \n    # Set initial epsilon for all agents\n    for agent in agents.agents:\n        agent.eps = initial_eps\n\n    for episode in range(1, num_episodes + 1):\n        state = env.reset()\n        agents.init_agents(state)\n\n        # Reset experience tracking for new episode\n        for i in range(agents.n_robots):\n            agents.prev_states[i] = None\n            agents.prev_actions[i] = None\n\n        # Decay epsilon\n        if episode <= eps_decay_episodes:\n            current_eps = initial_eps - (initial_eps - final_eps) * (episode / eps_decay_episodes)\n            for agent in agents.agents:\n                agent.eps = current_eps\n\n        # Run episode\n        done = False\n        # episode_reward = 0\n        episode_org_reward = 0\n        t = 0\n\n        while not done:\n            actions = agents.get_actions(state)\n            next_state, reward, done, infos = env.step(actions)\n            # shaped_reward = reward_shaping(reward, env, next_state, state, actions)\n            # print(f\"shaped reward {shaped_reward}\")\n            # Add reward to state for experience collection\n            # next_state['reward'] = shaped_reward\n            next_state['done'] = done\n\n            # Update state for next iteration\n            state = next_state\n            # episode_reward += shaped_reward\n            episode_org_reward += reward\n            t += 1\n\n            # max time steps\n            if t == 1000:\n                break\n\n        successful_deliveries = sum(1 for package in env.packages if package.status == 'delivered')\n                \n        original_rewards.append(episode_org_reward)\n        # episode_rewards.append(episode_reward)\n\n        # Print progress\n        if episode % print_freq == 0:\n            # avg_reward = sum(episode_rewards[-print_freq:]) / print_freq\n            avg_org_reward = sum(original_rewards[-print_freq:]) / print_freq\n            print(f\"Episode {episode}/{num_episodes}, \"f\"Avg Original Reward: {avg_org_reward:.2f}, \"f\"Epsilon: {agents.agents[0].eps:.3f}, \"f\"Buffer Size: {len(agents.shared_buffer)}\")\n        if wandb.run is not None:\n            wandb.log({\n                \"episode\": episode,\n                \"reward per episode\": episode_reward,\n                \"original reward per episode\": episode_org_reward,\n                \"epsilon\": agents.agents[0].eps,\n                \"buffer\": len(agents.shared_buffer),\n                \"successful_deliveries per episode\": successful_deliveries\n            })\n\n    return episode_rewards\n\n\ndef evaluate(env, agents, num_episodes=10):\n    \"\"\"\n    Evaluate trained agents without exploration\n\n    Args:\n        env: Environment to evaluate in\n        agents: Trained SEAC agents\n        num_episodes: Number of evaluation episodes\n\n    Returns:\n        float: Average reward per episode\n    \"\"\"\n    # Disable exploration and training\n    training_mode_backup = agents.training_mode\n    agents.training_mode = False\n\n    total_rewards = []\n    \n    for episode in range(num_episodes):\n        state = env.reset()\n        done = False\n        episode_reward = 0\n\n        while not done:\n            actions = agents.get_actions(state)\n            next_state, reward, done, _ = env.step(actions)\n\n            state = next_state\n            episode_reward += reward\n\n        total_rewards.append(episode_reward)\n        print(f\"Evaluation Episode {episode+1}/{num_episodes}, Reward: {episode_reward:.2f}\")\n\n    # Restore training mode\n    agents.training_mode = training_mode_backup\n\n    avg_reward = sum(total_rewards) / num_episodes\n    print(f\"Evaluation complete. Average Reward: {avg_reward:.2f}\")\n    return avg_reward","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:57:41.153077Z","iopub.execute_input":"2025-05-21T11:57:41.153700Z","iopub.status.idle":"2025-05-21T11:57:41.171499Z","shell.execute_reply.started":"2025-05-21T11:57:41.153673Z","shell.execute_reply":"2025-05-21T11:57:41.170295Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"maps = {\n    \"map1\": {\"map_file\": 'map1.txt',\"max_time_steps\": 100,'n_robots': 5, \"n_packages\": 100, 'seed': 10},\n    \"map2\": {\"map_file\": 'map2.txt',\"max_time_steps\": 1000,'n_robots': 5, \"n_packages\": 100, 'seed': 10},\n    \"map3\": {\"map_file\": 'map3.txt',\"max_time_steps\": 1000,'n_robots': 5, \"n_packages\": 500, 'seed': 10},\n    \"map4\": {\"map_file\": 'map4.txt',\"max_time_steps\": 1000,'n_robots': 10, \"n_packages\": 500, 'seed': 10},\n    \"map5\": {\"map_file\": 'map5.txt',\"max_time_steps\": 1000,'n_robots': 10, \"n_packages\": 100, 'seed': 10},\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:57:44.242891Z","iopub.execute_input":"2025-05-21T11:57:44.243332Z","iopub.status.idle":"2025-05-21T11:57:44.251078Z","shell.execute_reply.started":"2025-05-21T11:57:44.243299Z","shell.execute_reply":"2025-05-21T11:57:44.249878Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Import necessary modules\nfrom env import Environment\nimport os\nimport matplotlib.pyplot as plt\nfrom multiprocessing import Process, Queue\nimport ray\n\n# Create environment\nselected = 'map1'\nif selected == 'map1':\n    PATH = maps[selected]['map_file']\n    ROBOTS = maps[selected]['n_robots']\n    PACKAGES = maps[selected]['n_packages']\n    MAX_TIME_STEPS = 1000\n    SEED = 10\nelse:\n    print('Using map1 config')\n    PATH = 'map1.txt'\n    ROBOTS = 5\n    PACKAGES = 100\n    MAX_TIME_STEPS = 1000\n    SEED = 10\n\nenv = Environment(map_file=PATH, max_time_steps=MAX_TIME_STEPS, n_robots=ROBOTS, n_packages=PACKAGES, seed=SEED)\n# Initialize agents\nstate = env.reset()\nagents = Agents()\nagents.init_agents(state)\n\n# Train agents\nwandb.init(project=\"MARL-pickup-and-delivery\", name=\"SEAC\", \n           tags = ['map1'],\n           config={\n                \"num_episodes\": 250,\n                \"initial_eps\": 0.3,\n                \"final_eps\": 0.01,\n                \"eps_decay_episodes\": 250\n           })\n\nprint(\"Starting training...\")\nepisode_rewards = train(\n    env,\n    agents,\n    num_episodes=250,\n    print_freq=10,\n    initial_eps=0.3,\n    final_eps=0.01,\n    eps_decay_episodes=250\n)\n\n# Plot training progress\nplt.figure(figsize=(10, 5))\nplt.plot(episode_rewards)\nplt.title('SEAC Training Progress')\nplt.xlabel('Episode')\nplt.ylabel('Episode Reward')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-21T11:57:46.194896Z","iopub.execute_input":"2025-05-21T11:57:46.195304Z","iopub.status.idle":"2025-05-21T15:36:15.818624Z","shell.execute_reply.started":"2025-05-21T11:57:46.195273Z","shell.execute_reply":"2025-05-21T15:36:15.816880Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/marl-delivery/wandb/run-20250521_115751-6zrrtgn5</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/6zrrtgn5' target=\"_blank\">SEAC</a></strong> to <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery' target=\"_blank\">https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/6zrrtgn5' target=\"_blank\">https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/6zrrtgn5</a>"},"metadata":{}},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n","output_type":"stream"},{"name":"stdout","text":"Episode 10/250, Avg Shaped Reward: 2183.64, Avg Original Reward: -6.28, Epsilon: 0.288, Buffer Size: 5000\nEpisode 20/250, Avg Shaped Reward: 4198.42, Avg Original Reward: -8.70, Epsilon: 0.277, Buffer Size: 5000\nEpisode 30/250, Avg Shaped Reward: 7857.02, Avg Original Reward: -5.93, Epsilon: 0.265, Buffer Size: 5000\nEpisode 40/250, Avg Shaped Reward: 9969.98, Avg Original Reward: -6.65, Epsilon: 0.254, Buffer Size: 5000\nEpisode 50/250, Avg Shaped Reward: 8745.02, Avg Original Reward: -6.05, Epsilon: 0.242, Buffer Size: 5000\nEpisode 60/250, Avg Shaped Reward: 12749.23, Avg Original Reward: -3.69, Epsilon: 0.230, Buffer Size: 5000\nEpisode 70/250, Avg Shaped Reward: 4892.02, Avg Original Reward: -4.19, Epsilon: 0.219, Buffer Size: 5000\nEpisode 80/250, Avg Shaped Reward: 6130.30, Avg Original Reward: -4.90, Epsilon: 0.207, Buffer Size: 5000\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/4127279474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m episode_rewards = train(\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/1253480359.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(env, agents, num_episodes, print_freq, save_path, initial_eps, final_eps, eps_decay_episodes)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mshaped_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward_shaping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/306290530.py\u001b[0m in \u001b[0;36mget_actions\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_mode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_counter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrobots_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackages_before\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/306290530.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, robots_before, packages_before, batch_size)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Gradient clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stdout","text":"\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33mSEAC\u001b[0m at: \u001b[34mhttps://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/6zrrtgn5\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250521_115751-6zrrtgn5/logs\u001b[0m\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"agents.training_mode = False\nfor agent in agents.agents:\n    agent.eps = 0.0\nfor agent in agents.agents:\n    agent.actor.eval()\ndone = False\nstate = env.reset()\nt = 0\nreward_history = []\nwhile not done:\n    actions = agents.get_actions(state) \n    state, reward, done, infos = env.step(actions)\n\n    print(\"\\nState after step:\")\n    env.render()\n    print(f\"Reward: {reward}, Done: {done}, Infos: {infos}\")\n    print(\"Time step:\", env.t)\n    print(\"Packages:\", state['packages'])\n    print(\"Robots:\", state['robots'])\n    reward_history.append(env.total_reward)\n    # For debug purpose\n    t += 1\n    if t == 1000:\n        break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T04:06:27.918902Z","iopub.status.idle":"2025-05-13T04:06:27.919342Z","shell.execute_reply.started":"2025-05-13T04:06:27.919122Z","shell.execute_reply":"2025-05-13T04:06:27.919141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 5))\nplt.plot(reward_history, label=\"Total Reward\")\nplt.xlabel(\"Time Step\")\nplt.ylabel(\"Total Reward\")\nplt.title(\"Total Reward over Time\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T04:06:27.921340Z","iopub.status.idle":"2025-05-13T04:06:27.921726Z","shell.execute_reply.started":"2025-05-13T04:06:27.921532Z","shell.execute_reply":"2025-05-13T04:06:27.921547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T04:24:03.487333Z","iopub.execute_input":"2025-05-13T04:24:03.487690Z","iopub.status.idle":"2025-05-13T04:24:05.069833Z","shell.execute_reply.started":"2025-05-13T04:24:03.487656Z","shell.execute_reply":"2025-05-13T04:24:05.068983Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>buffer</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>episode</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>epsilon</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>original reward per episode</td><td>‚ñà‚ñÅ‚ñÑ‚ñÉ‚ñÉ</td></tr><tr><td>reward per episode</td><td>‚ñà‚ñÉ‚ñÖ‚ñÅ‚ñÑ</td></tr><tr><td>successful_deliveries per episode</td><td>‚ñÖ‚ñÅ‚ñà‚ñÖ‚ñÜ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>buffer</td><td>5000</td></tr><tr><td>episode</td><td>5</td></tr><tr><td>epsilon</td><td>0.2942</td></tr><tr><td>original reward per episode</td><td>-4.34</td></tr><tr><td>reward per episode</td><td>10675.05482</td></tr><tr><td>successful_deliveries per episode</td><td>6</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">SEAC</strong> at: <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/rmscedo7' target=\"_blank\">https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery/runs/rmscedo7</a><br> View project at: <a href='https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery' target=\"_blank\">https://wandb.ai/iai-uet-vnu/MARL-pickup-and-delivery</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>/kaggle/working/marl-delivery/wandb/run-20250513_040641-rmscedo7/logs</code>"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T06:02:04.790213Z","iopub.execute_input":"2025-05-13T06:02:04.790493Z","iopub.status.idle":"2025-05-13T06:02:04.879808Z","shell.execute_reply.started":"2025-05-13T06:02:04.790472Z","shell.execute_reply":"2025-05-13T06:02:04.878630Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2127913808.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"],"ename":"NameError","evalue":"name 'wandb' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}